{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2722570a-9d97-4814-8248-8465a8add9a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/Projects/ericlief/us-patent/.venv/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import csv\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# !jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "accff363-c021-44dd-8f55-aeab926ca687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d48c1b1a-8407-47de-ba1d-7e834fa4dc4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                               text\n",
       "0           tech  tv future in the hands of viewers with home th...\n",
       "1       business  worldcom boss  left books alone  former worldc...\n",
       "2          sport  tigers wary of farrell  gamble  leicester say ...\n",
       "3          sport  yeading face newcastle in fa cup premiership s...\n",
       "4  entertainment  ocean s twelve raids box office ocean s twelve..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_data_path = \"/home/liefe/Downloads/bbc-text.csv\"\n",
    "df = pd.read_csv(train_data_path)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12176dc2-b17f-45df-bd3c-a9a6de4f1f2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['tech', 'business', 'sport', 'entertainment', 'politics'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.unique(df.category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86f6dc63-fe1a-41a4-8c26-03651930b344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sport            511\n",
       "business         510\n",
       "politics         417\n",
       "tech             401\n",
       "entertainment    386\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d227e48-c705-4ccd-a01a-fb6ba22e8fc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business</th>\n",
       "      <th>entertainment</th>\n",
       "      <th>politics</th>\n",
       "      <th>sport</th>\n",
       "      <th>tech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2220</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2225 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      business  entertainment  politics  sport  tech\n",
       "0            0              0         0      0     1\n",
       "1            1              0         0      0     0\n",
       "2            0              0         0      1     0\n",
       "3            0              0         0      1     0\n",
       "4            0              1         0      0     0\n",
       "...        ...            ...       ...    ...   ...\n",
       "2220         1              0         0      0     0\n",
       "2221         0              0         1      0     0\n",
       "2222         0              1         0      0     0\n",
       "2223         0              0         1      0     0\n",
       "2224         0              0         0      1     0\n",
       "\n",
       "[2225 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.get_dummies(df[\"category\"])\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d85c379a-8c1f-448d-819e-043526e10953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>business</th>\n",
       "      <th>entertainment</th>\n",
       "      <th>politics</th>\n",
       "      <th>sport</th>\n",
       "      <th>tech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2220</th>\n",
       "      <td>business</td>\n",
       "      <td>cars pull down us retail figures us retail sal...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>politics</td>\n",
       "      <td>kilroy unveils immigration policy ex-chatshow ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>rem announce new glasgow concert us band rem h...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>politics</td>\n",
       "      <td>how political squabbles snowball it s become c...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>sport</td>\n",
       "      <td>souness delight at euro progress boss graeme s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2225 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           category                                               text  \\\n",
       "0              tech  tv future in the hands of viewers with home th...   \n",
       "1          business  worldcom boss  left books alone  former worldc...   \n",
       "2             sport  tigers wary of farrell  gamble  leicester say ...   \n",
       "3             sport  yeading face newcastle in fa cup premiership s...   \n",
       "4     entertainment  ocean s twelve raids box office ocean s twelve...   \n",
       "...             ...                                                ...   \n",
       "2220       business  cars pull down us retail figures us retail sal...   \n",
       "2221       politics  kilroy unveils immigration policy ex-chatshow ...   \n",
       "2222  entertainment  rem announce new glasgow concert us band rem h...   \n",
       "2223       politics  how political squabbles snowball it s become c...   \n",
       "2224          sport  souness delight at euro progress boss graeme s...   \n",
       "\n",
       "      business  entertainment  politics  sport  tech  \n",
       "0            0              0         0      0     1  \n",
       "1            1              0         0      0     0  \n",
       "2            0              0         0      1     0  \n",
       "3            0              0         0      1     0  \n",
       "4            0              1         0      0     0  \n",
       "...        ...            ...       ...    ...   ...  \n",
       "2220         1              0         0      0     0  \n",
       "2221         0              0         1      0     0  \n",
       "2222         0              1         0      0     0  \n",
       "2223         0              0         1      0     0  \n",
       "2224         0              0         0      1     0  \n",
       "\n",
       "[2225 rows x 7 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.concat([df, df2], axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef9159dc-9c47-4a96-ba14-37971fa29ad5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, ..., 3, 4, 2]),\n",
       " Index(['tech', 'business', 'sport', 'entertainment', 'politics'], dtype='object'))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.factorize(df[\"category\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8d883ef-0f7f-405d-bbff-c17eb496f3a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2220</th>\n",
       "      <td>1</td>\n",
       "      <td>cars pull down us retail figures us retail sal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>4</td>\n",
       "      <td>kilroy unveils immigration policy ex-chatshow ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>3</td>\n",
       "      <td>rem announce new glasgow concert us band rem h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>4</td>\n",
       "      <td>how political squabbles snowball it s become c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>2</td>\n",
       "      <td>souness delight at euro progress boss graeme s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2225 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      category                                               text\n",
       "0            0  tv future in the hands of viewers with home th...\n",
       "1            1  worldcom boss  left books alone  former worldc...\n",
       "2            2  tigers wary of farrell  gamble  leicester say ...\n",
       "3            2  yeading face newcastle in fa cup premiership s...\n",
       "4            3  ocean s twelve raids box office ocean s twelve...\n",
       "...        ...                                                ...\n",
       "2220         1  cars pull down us retail figures us retail sal...\n",
       "2221         4  kilroy unveils immigration policy ex-chatshow ...\n",
       "2222         3  rem announce new glasgow concert us band rem h...\n",
       "2223         4  how political squabbles snowball it s become c...\n",
       "2224         2  souness delight at euro progress boss graeme s...\n",
       "\n",
       "[2225 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"category\"] = pd.factorize(df[\"category\"])[0]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8f1311c-aa01-4d60-be7b-ed186b5b5679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['category', 'text'],\n",
       "    num_rows: 2225\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "ds = Dataset.from_pandas(df)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ee6db28-4214-452d-ba5c-6eeb62ebd597",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Get HF tokenizer, let's try this one:\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilroberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cef2ecfe-45cc-4baf-8152-585ee16bf102",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.63ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'category': 0, 'input_ids': [0, 18724, 499, 11, 5, 1420, 9, 5017, 19, 184, 8870, 1743, 1437, 29051, 239, 12, 38835, 326, 15597, 1437, 8, 1778, 569, 638, 268, 1375, 88, 5, 1207, 929, 1437, 5, 169, 82, 1183, 30016, 40, 28, 26396, 430, 11, 292, 107, 1437, 86, 4, 1437, 14, 16, 309, 7, 41, 3827, 2798, 61, 4366, 23, 5, 1013, 2267, 8917, 311, 11, 5573, 5030, 16306, 7, 2268, 141, 209, 92, 4233, 40, 913, 65, 9, 84, 5548, 375, 9452, 4, 19, 5, 201, 981, 5, 2904, 1437, 8864, 8, 97, 1383, 40, 28, 2781, 7, 5017, 1241, 184, 4836, 1437, 149, 6129, 1437, 7595, 1437, 9146, 29, 451, 1437, 8, 11451, 544, 4898, 7, 760, 5351, 8, 15295, 2110, 4, 1437, 65, 9, 5, 144, 3244, 12, 9006, 4233, 9, 740, 293, 34, 57, 1778, 8, 1081, 569, 638, 268, 36, 417, 37032, 8, 181, 37032, 322, 209, 278, 12, 8766, 7644, 1437, 101, 5, 201, 579, 326, 9697, 8, 5, 1717, 330, 579, 6360, 2744, 467, 1437, 1157, 82, 7, 638, 1437, 1400, 1437, 310, 1437, 13787, 8, 556, 2508, 30016, 8864, 77, 51, 236, 4, 1437, 5700, 1437, 5, 806, 2386, 13, 203, 55, 1081, 1720, 30016, 4, 51, 32, 67, 145, 1490, 12, 179, 7, 239, 12, 38835, 30016, 3880, 1437, 61, 32, 380, 265, 11, 1236, 20948, 8, 5, 201, 1437, 53, 9992, 7, 185, 160, 11, 2287, 2379, 142, 9, 5, 1762, 9, 239, 12, 38835, 8326, 4, 45, 129, 64, 82, 556, 2508, 149, 2329, 24038, 1437, 51, 64, 67, 4309, 59, 34900, 30, 1546, 8, 4238, 15579, 1437, 2057, 561, 49, 308, 10, 12, 2560, 12, 33663, 242, 4000, 4, 53, 103, 201, 4836, 8, 6129, 8, 7595, 451, 32, 3915, 59, 99, 24, 839, 13, 106, 11, 1110, 9, 4579, 3883, 25, 157, 25, 1437, 1518, 3599, 1437, 8, 18754, 10177, 7, 6237, 4, 1712, 5, 201, 3315, 11, 42, 806, 23, 5, 1151, 1437, 24, 16, 67, 10, 2212, 14, 16, 145, 1179, 11, 2287, 2379, 1437, 1605, 19, 5, 1197, 33646, 9, 518, 101, 6360, 27079, 1437, 99, 2594, 259, 452, 1437, 52, 40, 192, 11, 1117, 377, 7, 10, 107, 1437, 86, 11, 5, 1717, 330, 1437, 1437, 40587, 10080, 242, 1437, 5, 741, 23219, 2308, 579, 20136, 7367, 12376, 661, 174, 5, 741, 23219, 340, 998, 4, 13, 5, 3829, 9, 5, 741, 23219, 1437, 89, 32, 117, 743, 9, 685, 4579, 903, 648, 4, 24, 16, 10, 55, 10275, 696, 23, 5, 1151, 13, 1861, 1717, 330, 25106, 1437, 53, 1518, 10177, 16, 505, 13, 961, 4, 1437, 52, 40, 28, 1686, 55, 59, 1383, 3595, 1195, 87, 1546, 3595, 1437, 1437, 26, 15679, 1368, 260, 10849, 1437, 31, 1518, 4372, 933, 999, 175, 433, 13493, 4, 1437, 5, 2015, 16, 14, 19, 11451, 7070, 1437, 5670, 64, 28, 5, 3436, 9, 1383, 4, 1437, 37, 355, 35, 1437, 5, 1539, 122, 16, 14, 24, 16, 543, 7, 3720, 10, 3020, 19, 98, 203, 2031, 4, 1437, 1437, 99, 42, 839, 1437, 26, 1690, 10042, 1236, 1168, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Set up tokenizer to encode whole dataset\n",
    "ds = ds.map(lambda example: tokenizer(example[\"text\"], padding=\"max_length\", truncation=True),\n",
    "            batched=True,\n",
    "            remove_columns=[\"text\"])\n",
    "print(ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7a8c810-fea8-4a20-b99a-f5ca3e1256a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds[0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cbada7-6958-4674-b581-98b0a96f3e45",
   "metadata": {},
   "source": [
    "Thus the input has been truncated to max=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a822bf56-0a97-483d-9ed9-1dfad6ac7907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': 0, 'input_ids': [0, 18724, 499, 11, 5, 1420, 9, 5017, 19, 184, 8870, 1743, 1437, 29051, 239, 12, 38835, 326, 15597, 1437, 8, 1778, 569, 638, 268, 1375, 88, 5, 1207, 929, 1437, 5, 169, 82, 1183, 30016, 40, 28, 26396, 430, 11, 292, 107, 1437, 86, 4, 1437, 14, 16, 309, 7, 41, 3827, 2798, 61, 4366, 23, 5, 1013, 2267, 8917, 311, 11, 5573, 5030, 16306, 7, 2268, 141, 209, 92, 4233, 40, 913, 65, 9, 84, 5548, 375, 9452, 4, 19, 5, 201, 981, 5, 2904, 1437, 8864, 8, 97, 1383, 40, 28, 2781, 7, 5017, 1241, 184, 4836, 1437, 149, 6129, 1437, 7595, 1437, 9146, 29, 451, 1437, 8, 11451, 544, 4898, 7, 760, 5351, 8, 15295, 2110, 4, 1437, 65, 9, 5, 144, 3244, 12, 9006, 4233, 9, 740, 293, 34, 57, 1778, 8, 1081, 569, 638, 268, 36, 417, 37032, 8, 181, 37032, 322, 209, 278, 12, 8766, 7644, 1437, 101, 5, 201, 579, 326, 9697, 8, 5, 1717, 330, 579, 6360, 2744, 467, 1437, 1157, 82, 7, 638, 1437, 1400, 1437, 310, 1437, 13787, 8, 556, 2508, 30016, 8864, 77, 51, 236, 4, 1437, 5700, 1437, 5, 806, 2386, 13, 203, 55, 1081, 1720, 30016, 4, 51, 32, 67, 145, 1490, 12, 179, 7, 239, 12, 38835, 30016, 3880, 1437, 61, 32, 380, 265, 11, 1236, 20948, 8, 5, 201, 1437, 53, 9992, 7, 185, 160, 11, 2287, 2379, 142, 9, 5, 1762, 9, 239, 12, 38835, 8326, 4, 45, 129, 64, 82, 556, 2508, 149, 2329, 24038, 1437, 51, 64, 67, 4309, 59, 34900, 30, 1546, 8, 4238, 15579, 1437, 2057, 561, 49, 308, 10, 12, 2560, 12, 33663, 242, 4000, 4, 53, 103, 201, 4836, 8, 6129, 8, 7595, 451, 32, 3915, 59, 99, 24, 839, 13, 106, 11, 1110, 9, 4579, 3883, 25, 157, 25, 1437, 1518, 3599, 1437, 8, 18754, 10177, 7, 6237, 4, 1712, 5, 201, 3315, 11, 42, 806, 23, 5, 1151, 1437, 24, 16, 67, 10, 2212, 14, 16, 145, 1179, 11, 2287, 2379, 1437, 1605, 19, 5, 1197, 33646, 9, 518, 101, 6360, 27079, 1437, 99, 2594, 259, 452, 1437, 52, 40, 192, 11, 1117, 377, 7, 10, 107, 1437, 86, 11, 5, 1717, 330, 1437, 1437, 40587, 10080, 242, 1437, 5, 741, 23219, 2308, 579, 20136, 7367, 12376, 661, 174, 5, 741, 23219, 340, 998, 4, 13, 5, 3829, 9, 5, 741, 23219, 1437, 89, 32, 117, 743, 9, 685, 4579, 903, 648, 4, 24, 16, 10, 55, 10275, 696, 23, 5, 1151, 13, 1861, 1717, 330, 25106, 1437, 53, 1518, 10177, 16, 505, 13, 961, 4, 1437, 52, 40, 28, 1686, 55, 59, 1383, 3595, 1195, 87, 1546, 3595, 1437, 1437, 26, 15679, 1368, 260, 10849, 1437, 31, 1518, 4372, 933, 999, 175, 433, 13493, 4, 1437, 5, 2015, 16, 14, 19, 11451, 7070, 1437, 5670, 64, 28, 5, 3436, 9, 1383, 4, 1437, 37, 355, 35, 1437, 5, 1539, 122, 16, 14, 24, 16, 543, 7, 3720, 10, 3020, 19, 98, 203, 2031, 4, 1437, 1437, 99, 42, 839, 1437, 26, 1690, 10042, 1236, 1168, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# Remove unnecessary columns/can also set above\n",
    "ds = ds.rename_column(\"category\", \"labels\")\n",
    "print(ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "250d9769-7247-41be-bb24-f576c21b4c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': tensor(0, device='cuda:0'), 'input_ids': tensor([    0, 18724,   499,    11,     5,  1420,     9,  5017,    19,   184,\n",
      "         8870,  1743,  1437, 29051,   239,    12, 38835,   326, 15597,  1437,\n",
      "            8,  1778,   569,   638,   268,  1375,    88,     5,  1207,   929,\n",
      "         1437,     5,   169,    82,  1183, 30016,    40,    28, 26396,   430,\n",
      "           11,   292,   107,  1437,    86,     4,  1437,    14,    16,   309,\n",
      "            7,    41,  3827,  2798,    61,  4366,    23,     5,  1013,  2267,\n",
      "         8917,   311,    11,  5573,  5030, 16306,     7,  2268,   141,   209,\n",
      "           92,  4233,    40,   913,    65,     9,    84,  5548,   375,  9452,\n",
      "            4,    19,     5,   201,   981,     5,  2904,  1437,  8864,     8,\n",
      "           97,  1383,    40,    28,  2781,     7,  5017,  1241,   184,  4836,\n",
      "         1437,   149,  6129,  1437,  7595,  1437,  9146,    29,   451,  1437,\n",
      "            8, 11451,   544,  4898,     7,   760,  5351,     8, 15295,  2110,\n",
      "            4,  1437,    65,     9,     5,   144,  3244,    12,  9006,  4233,\n",
      "            9,   740,   293,    34,    57,  1778,     8,  1081,   569,   638,\n",
      "          268,    36,   417, 37032,     8,   181, 37032,   322,   209,   278,\n",
      "           12,  8766,  7644,  1437,   101,     5,   201,   579,   326,  9697,\n",
      "            8,     5,  1717,   330,   579,  6360,  2744,   467,  1437,  1157,\n",
      "           82,     7,   638,  1437,  1400,  1437,   310,  1437, 13787,     8,\n",
      "          556,  2508, 30016,  8864,    77,    51,   236,     4,  1437,  5700,\n",
      "         1437,     5,   806,  2386,    13,   203,    55,  1081,  1720, 30016,\n",
      "            4,    51,    32,    67,   145,  1490,    12,   179,     7,   239,\n",
      "           12, 38835, 30016,  3880,  1437,    61,    32,   380,   265,    11,\n",
      "         1236, 20948,     8,     5,   201,  1437,    53,  9992,     7,   185,\n",
      "          160,    11,  2287,  2379,   142,     9,     5,  1762,     9,   239,\n",
      "           12, 38835,  8326,     4,    45,   129,    64,    82,   556,  2508,\n",
      "          149,  2329, 24038,  1437,    51,    64,    67,  4309,    59, 34900,\n",
      "           30,  1546,     8,  4238, 15579,  1437,  2057,   561,    49,   308,\n",
      "           10,    12,  2560,    12, 33663,   242,  4000,     4,    53,   103,\n",
      "          201,  4836,     8,  6129,     8,  7595,   451,    32,  3915,    59,\n",
      "           99,    24,   839,    13,   106,    11,  1110,     9,  4579,  3883,\n",
      "           25,   157,    25,  1437,  1518,  3599,  1437,     8, 18754, 10177,\n",
      "            7,  6237,     4,  1712,     5,   201,  3315,    11,    42,   806,\n",
      "           23,     5,  1151,  1437,    24,    16,    67,    10,  2212,    14,\n",
      "           16,   145,  1179,    11,  2287,  2379,  1437,  1605,    19,     5,\n",
      "         1197, 33646,     9,   518,   101,  6360, 27079,  1437,    99,  2594,\n",
      "          259,   452,  1437,    52,    40,   192,    11,  1117,   377,     7,\n",
      "           10,   107,  1437,    86,    11,     5,  1717,   330,  1437,  1437,\n",
      "        40587, 10080,   242,  1437,     5,   741, 23219,  2308,   579, 20136,\n",
      "         7367, 12376,   661,   174,     5,   741, 23219,   340,   998,     4,\n",
      "           13,     5,  3829,     9,     5,   741, 23219,  1437,    89,    32,\n",
      "          117,   743,     9,   685,  4579,   903,   648,     4,    24,    16,\n",
      "           10,    55, 10275,   696,    23,     5,  1151,    13,  1861,  1717,\n",
      "          330, 25106,  1437,    53,  1518, 10177,    16,   505,    13,   961,\n",
      "            4,  1437,    52,    40,    28,  1686,    55,    59,  1383,  3595,\n",
      "         1195,    87,  1546,  3595,  1437,  1437,    26, 15679,  1368,   260,\n",
      "        10849,  1437,    31,  1518,  4372,   933,   999,   175,   433, 13493,\n",
      "            4,  1437,     5,  2015,    16,    14,    19, 11451,  7070,  1437,\n",
      "         5670,    64,    28,     5,  3436,     9,  1383,     4,  1437,    37,\n",
      "          355,    35,  1437,     5,  1539,   122,    16,    14,    24,    16,\n",
      "          543,     7,  3720,    10,  3020,    19,    98,   203,  2031,     4,\n",
      "         1437,  1437,    99,    42,   839,  1437,    26,  1690, 10042,  1236,\n",
      "         1168,     2], device='cuda:0'), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "# Convert to torch tensors.\n",
    "# Strange things happen if you forget this, like time major ds\n",
    "ds.set_format(type=\"torch\", device=device)\n",
    "print(ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "264b9791-3d17-4ba5-b220-7e31dddf3e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.train_test_split(.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4858ccb8-a89e-4880-b1e9-f2d529961c80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 2002\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 223\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4df889b-95c1-4ed4-93ca-38116b59cf6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': <torch.utils.data.dataloader.DataLoader at 0x7fe9f477f850>,\n",
       " 'test': <torch.utils.data.dataloader.DataLoader at 0x7fe9f47f99c0>}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloaders = {partition: DataLoader(ds[partition], batch_size=8, shuffle=True) for partition in ds.keys()}\n",
    "dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01c0af51-9a18-4180-a6e1-d37193227558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values([tensor([3, 4, 0, 0, 2, 4, 0, 2], device='cuda:0'), tensor([[    0,   687,  4440,  ...,     1,     1,     1],\n",
      "        [    0,  2413, 12778,  ...,  3825,    11,     2],\n",
      "        [    0,  4291,  2607,  ...,    16, 11190,     2],\n",
      "        ...,\n",
      "        [    0,  4651,  2767,  ...,    32,   103,     2],\n",
      "        [    0, 36436, 20654,  ...,     1,     1,     1],\n",
      "        [    0,  4651, 24773,  ...,    35, 28041,     2]], device='cuda:0'), tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')])\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloaders[\"train\"]:\n",
    "    print(batch.values())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c415580-cede-445c-8b89-4459e46c0966",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=5, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilroberta-base\", num_labels=5)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "483d1703-ee40-4f16-9d8e-ab4e5e48300e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/Projects/ericlief/us-patent/.venv/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "# Set weight decay on params\n",
    "names_params = list(model.named_parameters())\n",
    "no_decay = (\"bias\", \"gamma\", \"beta\")\n",
    "optimizer_grouped_params = (\n",
    "    dict(params=(p for n, p in names_params if not any(nd in n for nd in no_decay)),\n",
    "         weight_decay=0.01),\n",
    "    dict(params=(p for n, p in names_params if any(nd in n for nd in no_decay)),\n",
    "         weight_decay=0.0)\n",
    ")\n",
    "\n",
    "lr = 5e-5\n",
    "optimizer = AdamW(optimizer_grouped_params, lr=lr, correct_bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "209fbadf-4dcd-4791-8d80-c4a1ac6a2397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "# def compute_metrics(p):    \n",
    "#     pred, labels = p\n",
    "#     pred = np.argmax(pred, axis=1)\n",
    "#     accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
    "#     recall = recall_score(y_true=labels, y_pred=pred)\n",
    "#     precision = precision_score(y_true=labels, y_pred=pred)\n",
    "#     f1 = f1_score(y_true=labels, y_pred=pred)    \n",
    "#     return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2580b767-9cab-4e80-81ed-6057ef3df7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/Projects/ericlief/us-patent/.venv/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2002\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 24\n",
      "  Gradient Accumulation steps = 3\n",
      "  Total optimization steps = 249\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='249' max='249' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [249/249 03:08, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.521323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.098699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.018713</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 223\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 223\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 223\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=249, training_loss=0.7996157604049009, metrics={'train_runtime': 189.3994, 'train_samples_per_second': 31.711, 'train_steps_per_second': 1.315, 'total_flos': 794317016616960.0, 'train_loss': 0.7996157604049009, 'epoch': 2.99})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "args = TrainingArguments(output_dir=\".\",\n",
    "                         per_device_train_batch_size=8,\n",
    "                         per_device_eval_batch_size=8,\n",
    "                         dataloader_pin_memory=False,\n",
    "                         evaluation_strategy=\"epoch\",\n",
    "                         gradient_accumulation_steps=3,\n",
    "                         learning_rate=5e-5,\n",
    "                         warmup_steps=1000,\n",
    "                         optim=\"adamw_hf\")\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                  args=args,\n",
    "                  train_dataset=ds[\"train\"],\n",
    "                  eval_dataset=ds[\"test\"],\n",
    "                  #compute_metrics=compute_metrics\n",
    "                 )\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b8f5c62-c949-469b-a575-a9c4746ed01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.evaluate(ds[\"test\"], metric_key_prefix=\"eval_acc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "beae5bd3-874b-47ff-8f9d-d63e59f01cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 223\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='28' max='28' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [28/28 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "out = trainer.predict(ds[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "45284616-c9c6-4af9-af81-3b8151b22186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[-1.454209  ,  5.1654735 , -1.1436566 , -1.540897  , -1.344166  ],\n",
       "       [ 0.45798662,  4.688294  , -1.6425015 , -1.6239082 , -1.738363  ],\n",
       "       [-1.383109  , -1.576714  ,  5.087396  , -1.1475655 , -0.9978528 ],\n",
       "       ...,\n",
       "       [-1.2850131 , -0.9422651 , -1.4965856 , -1.3963528 ,  4.7641954 ],\n",
       "       [-0.9424531 , -1.325981  , -1.2116297 ,  4.647848  , -1.041074  ],\n",
       "       [ 5.0915403 , -1.0789853 , -1.3039528 , -1.0373774 , -1.2336445 ]],\n",
       "      dtype=float32), label_ids=array([1, 1, 2, 0, 0, 3, 1, 1, 4, 3, 4, 2, 1, 4, 2, 4, 3, 1, 0, 3, 3, 3,\n",
       "       2, 1, 4, 1, 1, 0, 0, 0, 1, 3, 2, 0, 2, 1, 1, 0, 4, 0, 3, 3, 1, 1,\n",
       "       3, 2, 0, 3, 1, 1, 1, 2, 1, 2, 2, 1, 1, 4, 2, 3, 0, 0, 2, 3, 4, 4,\n",
       "       4, 2, 4, 1, 4, 1, 3, 2, 3, 3, 0, 3, 3, 1, 3, 1, 0, 1, 3, 2, 0, 0,\n",
       "       1, 0, 2, 4, 3, 4, 3, 3, 0, 1, 1, 3, 2, 1, 4, 1, 1, 1, 1, 3, 2, 3,\n",
       "       4, 2, 4, 2, 2, 3, 2, 2, 4, 3, 0, 3, 2, 0, 4, 2, 1, 4, 0, 3, 2, 3,\n",
       "       2, 1, 4, 2, 4, 1, 3, 1, 1, 4, 1, 1, 4, 1, 2, 4, 4, 0, 1, 1, 4, 2,\n",
       "       2, 0, 1, 3, 2, 4, 2, 0, 4, 0, 1, 2, 3, 4, 4, 3, 1, 4, 1, 1, 1, 4,\n",
       "       4, 1, 1, 3, 1, 2, 4, 0, 2, 2, 1, 3, 0, 2, 0, 2, 2, 1, 2, 0, 3, 1,\n",
       "       0, 3, 2, 0, 1, 4, 2, 0, 3, 4, 0, 0, 1, 4, 2, 0, 4, 1, 3, 2, 0, 1,\n",
       "       4, 3, 0]), metrics={'test_loss': 0.01871279440820217, 'test_runtime': 2.3607, 'test_samples_per_second': 94.463, 'test_steps_per_second': 11.861})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2f8be26b-1554-4aa7-b624-7627137c4950",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, y_true, _  = out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cfb11948-2ec9-48a1-8c16-3bb528eefbcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.454209  ,  5.1654735 , -1.1436566 , -1.540897  , -1.344166  ],\n",
       "       [ 0.45798662,  4.688294  , -1.6425015 , -1.6239082 , -1.738363  ],\n",
       "       [-1.383109  , -1.576714  ,  5.087396  , -1.1475655 , -0.9978528 ],\n",
       "       ...,\n",
       "       [-1.2850131 , -0.9422651 , -1.4965856 , -1.3963528 ,  4.7641954 ],\n",
       "       [-0.9424531 , -1.325981  , -1.2116297 ,  4.647848  , -1.041074  ],\n",
       "       [ 5.0915403 , -1.0789853 , -1.3039528 , -1.0373774 , -1.2336445 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "539d7525-adbf-42ba-8f1e-0a7baf67b4e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 2, 0, 0, 3, 1, 1, 4, 3, 4, 2, 1, 4, 2, 4, 3, 1, 0, 3, 3, 3,\n",
       "       2, 1, 4, 1, 1, 0, 0, 0, 1, 3, 2, 0, 2, 1, 1, 0, 4, 0, 3, 3, 1, 1,\n",
       "       3, 2, 0, 3, 1, 1, 1, 2, 1, 2, 2, 1, 1, 4, 2, 3, 0, 0, 2, 3, 4, 4,\n",
       "       4, 2, 4, 1, 4, 1, 3, 2, 3, 3, 0, 3, 3, 1, 3, 1, 0, 1, 3, 2, 0, 0,\n",
       "       1, 0, 2, 4, 3, 4, 3, 3, 0, 1, 1, 3, 2, 1, 4, 1, 1, 1, 1, 3, 2, 3,\n",
       "       4, 2, 4, 2, 2, 3, 2, 2, 4, 3, 0, 3, 2, 0, 4, 2, 1, 4, 0, 3, 2, 3,\n",
       "       2, 1, 4, 2, 4, 1, 3, 1, 1, 4, 1, 1, 4, 1, 2, 4, 4, 0, 1, 1, 4, 2,\n",
       "       2, 0, 1, 3, 2, 4, 2, 0, 4, 0, 1, 2, 3, 4, 4, 3, 1, 4, 1, 1, 1, 4,\n",
       "       4, 1, 1, 3, 1, 2, 4, 0, 2, 2, 1, 3, 0, 2, 0, 2, 1, 1, 2, 0, 3, 1,\n",
       "       0, 3, 2, 0, 1, 4, 2, 0, 3, 4, 0, 0, 1, 4, 2, 0, 4, 1, 3, 2, 0, 1,\n",
       "       4, 3, 0])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "404dccee-4dfe-4929-abbf-8cfbfe351362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[37,  0,  0,  0,  0],\n",
       "       [ 0, 59,  0,  0,  0],\n",
       "       [ 0,  1, 44,  0,  0],\n",
       "       [ 0,  0,  0, 42,  0],\n",
       "       [ 0,  0,  0,  0, 40]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "71640013-f087-4766-828e-18c01a7ef4c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'              precision    recall  f1-score   support\\n\\n           0       1.00      1.00      1.00        37\\n           1       0.98      1.00      0.99        59\\n           2       1.00      0.98      0.99        45\\n           3       1.00      1.00      1.00        42\\n           4       1.00      1.00      1.00        40\\n\\n    accuracy                           1.00       223\\n   macro avg       1.00      1.00      1.00       223\\nweighted avg       1.00      1.00      1.00       223\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_report(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bb3ef9-450d-4312-a619-ecf7dce2f8b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (patent)",
   "language": "python",
   "name": "patent"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
